## Transformer

Attention is all you need 2017
1. 概率论
2. RNN、LSTM
3. Seq2Seq
4. 注意力机制

目标：
1. WMT数据集
2. self-atttention 机制
3. Belu评价标准
4. position encoding
5. layer normalization

### 背景介绍

WMT翻译数据集：包括德语翻译成英语、法语翻译成英语等数据集。数据集级量级在百万级别。

bleu采用了一种N-gram的匹配规则，去比较议文和参考译文n组词的相似比。 

#### 研究背景

#### 论文泛读

#### seq2seq attention

### 论文总览 Transformer结构

### self-attention

### 小trick

### 论文总结

### 代码实现分析

